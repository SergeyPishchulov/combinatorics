{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(202)\n",
    "from bitarray import bitarray, bits2bytes\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Jack_London_-_The_Sea_Wolf_ascii.txt', 'r') as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation based on https://www.cs.helsinki.fi/u/tpkarkka/publications/jacm05-revised.pdf and https://mailund.dk/posts/skew-python-go/\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def merge(x: np.array, SA12: np.array, SA3: np.array) -> np.array:\n",
    "    \"Merge the suffixes in sorted SA12 and SA3.\"\n",
    "    ISA = np.zeros((len(x),), dtype='int')\n",
    "    for i in range(len(SA12)):\n",
    "        ISA[SA12[i]] = i\n",
    "    SA = np.zeros((len(x),), dtype='int')\n",
    "    idx = 0\n",
    "    i, j = 0, 0\n",
    "    while i < len(SA12) and j < len(SA3):\n",
    "        if less(x, SA12[i], SA3[j], ISA):\n",
    "            SA[idx] = SA12[i]\n",
    "            idx += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            SA[idx] = SA3[j]\n",
    "            idx += 1\n",
    "            j += 1\n",
    "    if i < len(SA12):\n",
    "        SA[idx:len(SA)] = SA12[i:]\n",
    "    elif j < len(SA3):\n",
    "        SA[idx:len(SA)] = SA3[j:]\n",
    "    return SA\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def u_idx(i: int, m: int) -> int:\n",
    "    \"Map indices in u back to indices in the original string.\"\n",
    "    if i < m:\n",
    "        return 1 + 3 * i\n",
    "    else:\n",
    "        return 2 + 3 * (i - m - 1)\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def safe_idx(x: np.array, i: int) -> int:\n",
    "    \"Hack to get zero if we index beyond the end.\"\n",
    "    return 0 if i >= len(x) else x[i]\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def symbcount(x: np.array, asize: int) -> np.array:\n",
    "    \"Count how often we see each character in the alphabet.\"\n",
    "    counts = np.zeros((asize,), dtype=\"int\")\n",
    "    for c in x:\n",
    "        counts[c] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def cumsum(counts: np.array) -> np.array:\n",
    "    \"Compute the cumulative sum from the character count.\"\n",
    "    res = np.zeros((len(counts, )), dtype='int')\n",
    "    acc = 0\n",
    "    for i, k in enumerate(counts):\n",
    "        res[i] = acc\n",
    "        acc += k\n",
    "    return res\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def bucket_sort(x: np.array, asize: int,\n",
    "                idx: np.array, offset: int = 0) -> np.array:\n",
    "    \"Sort indices in idx according to x[i + offset].\"\n",
    "    sort_symbs = np.array([safe_idx(x, i + offset) for i in idx])\n",
    "    counts = symbcount(sort_symbs, asize)\n",
    "    buckets = cumsum(counts)\n",
    "    out = np.zeros((len(idx),), dtype='int')\n",
    "    for i in idx:\n",
    "        bucket = safe_idx(x, i + offset)\n",
    "        out[buckets[bucket]] = i\n",
    "        buckets[bucket] += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def radix3(x: np.array, asize: int, idx: np.array) -> np.array:\n",
    "    \"Sort indices in idx according to their first three letters in x.\"\n",
    "    idx = bucket_sort(x, asize, idx, 2)\n",
    "    idx = bucket_sort(x, asize, idx, 1)\n",
    "    return bucket_sort(x, asize, idx)\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def triplet(x: np.array, i: int) -> Tuple[int, int, int]:\n",
    "    \"Extract the triplet (x[i],x[i+1],x[i+2]).\"\n",
    "    return safe_idx(x, i), safe_idx(x, i + 1), safe_idx(x, i + 2)\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def collect_alphabet(x: np.array, idx: np.array) -> Tuple[np.array, int]:\n",
    "    \"Map the triplets starting at idx to a new alphabet.\"\n",
    "    alpha = np.zeros((len(x),), dtype='int')\n",
    "    value = 1\n",
    "    last_trip = -1, -1, -1\n",
    "    for i in idx:\n",
    "        trip = triplet(x, i)\n",
    "        if trip != last_trip:\n",
    "            value += 1\n",
    "            last_trip = trip\n",
    "        alpha[i] = value\n",
    "    return alpha, value - 1\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def build_u(x: np.array, alpha: np.array) -> np.array:\n",
    "    \"Construct u string, using 1 as central sentinel.\"\n",
    "    a = np.array([alpha[i] for i in range(1, len(x), 3)] +\n",
    "                 [1] +\n",
    "                 [alpha[i] for i in range(2, len(x), 3)])\n",
    "    return a\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def less(x: np.array, i: int, j: int, ISA: np.array) -> bool:\n",
    "    \"Check if x[i:] < x[j:] using the inverse suffix array for SA12.\"\n",
    "    a: int = safe_idx(x, i)\n",
    "    b: int = safe_idx(x, j)\n",
    "    if a < b:\n",
    "        return True\n",
    "    if a > b:\n",
    "        return False\n",
    "    if i % 3 != 0 and j % 3 != 0:\n",
    "        return ISA[i] < ISA[j]\n",
    "    return less(x, i + 1, j + 1, ISA)\n",
    "\n",
    "\n",
    "@numba.jit()\n",
    "def skew_rec(x: np.array, asize: int) -> np.array:\n",
    "    \"skew/DC3 SA construction algorithm.\"\n",
    "\n",
    "    SA12 = np.array([i for i in range(len(x)) if i % 3 != 0])\n",
    "\n",
    "    SA12 = radix3(x, asize, SA12)\n",
    "    new_alpha, new_asize = collect_alphabet(x, SA12)\n",
    "    if new_asize < len(SA12):\n",
    "        # Recursively sort SA12\n",
    "        u = build_u(x, new_alpha)\n",
    "        sa_u = skew_rec(u, new_asize + 2)\n",
    "        m = len(sa_u) // 2\n",
    "        SA12 = np.array([u_idx(i, m) for i in sa_u if i != m])\n",
    "\n",
    "    if len(x) % 3 == 1:\n",
    "        SA3 = np.array([len(x) - 1] + [i - 1 for i in SA12 if i % 3 == 1])\n",
    "    else:\n",
    "        SA3 = np.array([i - 1 for i in SA12 if i % 3 == 1])\n",
    "    SA3 = bucket_sort(x, asize, SA3)\n",
    "    return merge(x, SA12, SA3)\n",
    "\n",
    "\n",
    "def get_suffix_array(x: str) -> np.array:\n",
    "    if \"$\" in x:\n",
    "        raise ValueError('Text should not contain $')\n",
    "    str_to_int = {\n",
    "        \"$\": 0,  # End of strig\n",
    "    }\n",
    "    str_to_int = str_to_int | {\n",
    "        c: n+1\n",
    "        for (n, c) in enumerate(sorted(list(set(x))))\n",
    "    }\n",
    "    return skew_rec(np.array([str_to_int[y] for y in x]), len(str_to_int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sa(sa, t):\n",
    "    for x in sa:\n",
    "        print(t[x:]+t[0:x])\n",
    "    print('-'*10)\n",
    "\n",
    "\n",
    "dna_string = 'AGCTN4ACTGN'\n",
    "suffix_array = get_suffix_array(dna_string)\n",
    "# print_sa(suffix_array, dna_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_canon_repr(s):\n",
    "    \"\"\"Returns cononical representation of sort by string s\n",
    "    e.g. [3,1,0,2]\"\"\"\n",
    "    sort_info = [None]*len(s)\n",
    "    for new_place, (c, old_place) in enumerate(sorted([(c, i) for i, c\n",
    "                                                       in enumerate(s)])):\n",
    "        sort_info[old_place] = new_place\n",
    "    return sort_info\n",
    "\n",
    "\n",
    "def apply_permutation(s, perm):\n",
    "    res = [None]*len(s)\n",
    "    for old_place, new_place in enumerate(perm):\n",
    "        res[new_place] = s[old_place]\n",
    "    return res\n",
    "\n",
    "\n",
    "def inverse_permutation(canon_repr):\n",
    "    res = [None]*len(canon_repr)\n",
    "    for old_place, new_place in enumerate(canon_repr):\n",
    "        res[new_place] = old_place\n",
    "    return res\n",
    "\n",
    "\n",
    "ban_bwt = 'а#ннБннБаааа'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bwt & mtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARP = '#'\n",
    "\n",
    "\n",
    "class BWT:\n",
    "    def encode(t: str):\n",
    "        if SHARP not in t:\n",
    "            raise ValueError(f\"{SHARP}  is not found in text\")\n",
    "        bwt = [None]*len(t)\n",
    "        sa = get_suffix_array(t)\n",
    "        # print_sa(sa, t)\n",
    "        for i in range(len(t)):\n",
    "            bwt[i] = t[sa[i]-1]\n",
    "        return ''.join(bwt)\n",
    "\n",
    "    def decode(bwt: str):\n",
    "        sigma = get_sort_canon_repr(bwt)\n",
    "        inversed_sigma = inverse_permutation(sigma)\n",
    "        res = [None]*len(bwt)\n",
    "        i = bwt.index(SHARP)\n",
    "        index_in_first_col = inversed_sigma[i]\n",
    "        for j, c in enumerate(bwt):\n",
    "            res[j] = bwt[index_in_first_col]\n",
    "            index_in_first_col = inversed_sigma[index_in_first_col]\n",
    "        return ''.join(res)\n",
    "\n",
    "\n",
    "def _shift(alphabet, up, lo):\n",
    "    for i in range(lo, up-1, -1):\n",
    "        alphabet[i+1] = alphabet[i]\n",
    "    return alphabet\n",
    "\n",
    "\n",
    "class mtf:\n",
    "    def get_alphabet():\n",
    "        # return list('#Ban')\n",
    "        return [chr(i) for i in range(ord('z')+1)]\n",
    "\n",
    "    def update_alphabet(alphabet, ind, c):\n",
    "        if ind > 1:\n",
    "            _shift(alphabet, 1, ind-1)\n",
    "            alphabet[1] = c\n",
    "        if ind == 1:\n",
    "            alphabet[1] = alphabet[0]\n",
    "            alphabet[0] = c\n",
    "\n",
    "    def encode(t: str):\n",
    "        alphabet = mtf.get_alphabet()\n",
    "        diff = set(t)-set(alphabet)\n",
    "        if diff:\n",
    "            raise ValueError(\n",
    "                f'Found chars in text that are not presented in alphabet: {diff}')\n",
    "        res = []\n",
    "        for c in t:\n",
    "            ind = alphabet.index(c)\n",
    "            res.append(ind)\n",
    "            mtf.update_alphabet(alphabet, ind, c)\n",
    "        print(f\"mtf res max & min: {max(res), min(res)}\")\n",
    "        return res\n",
    "\n",
    "    def decode(encoded):# list of indecies\n",
    "        alphabet = mtf.get_alphabet()\n",
    "        res = []\n",
    "        diff = set(encoded) - set(range(len(alphabet)))\n",
    "        if diff:\n",
    "            raise ValueError(f\"Found wrong indecies in encoded by mtf: {diff}\")\n",
    "        for ind in encoded:\n",
    "            c = alphabet[ind]\n",
    "            res.append(c)\n",
    "            mtf.update_alphabet(alphabet, ind, c)\n",
    "        return ''.join(res)\n",
    "\n",
    "\n",
    "class rle:\n",
    "    def encode(ar):\n",
    "        res = []\n",
    "        cntrs = []\n",
    "        prev_is_zero = False\n",
    "        for x in ar:\n",
    "            if x != 0:\n",
    "                res.append(x)\n",
    "                prev_is_zero = False\n",
    "                continue\n",
    "            if prev_is_zero:\n",
    "                cntrs[-1] += 1\n",
    "            else:\n",
    "                res.append(0)\n",
    "                cntrs.append(1)\n",
    "            prev_is_zero = True\n",
    "        # print(f\"max in cntrs = {max(cntrs)}\")\n",
    "        return res, cntrs\n",
    "\n",
    "    def decode(rle_encoded, cntrs):\n",
    "        res = []\n",
    "        for x in rle_encoded:\n",
    "            if x != 0:\n",
    "                res.append(x)\n",
    "                continue\n",
    "            res.extend([0]*cntrs.pop(0))\n",
    "        return res\n",
    "    \n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def random_subtext(text, length):\n",
    "    i = random.randrange(0, len(text) - length - 2)\n",
    "    return text[i:i+length]\n",
    "\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def test_bwt_encode_decode():\n",
    "    for i in range(1000):\n",
    "        w = random_subtext(text, 1000)+SHARP\n",
    "        # print(w)\n",
    "        assert BWT.decode(BWT.encode(w)) == w\n",
    "\n",
    "\n",
    "def test_mtf():\n",
    "    for i in range(1000):\n",
    "        w = random_subtext(text, 1000)+SHARP\n",
    "        # print(w)\n",
    "        assert mtf.decode(mtf.encode(w)) == w\n",
    "\n",
    "\n",
    "def test_rle():\n",
    "    for i in range(100):\n",
    "        w = random_subtext(text, 1000)+SHARP\n",
    "        ar = mtf.encode(BWT.encode(w))\n",
    "        # print(ar)\n",
    "        assert rle.decode(*rle.encode(ar)) == ar\n",
    "\n",
    "\n",
    "# test_bwt_encode_decode()\n",
    "# test_mtf()\n",
    "# test_rle()\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def test_permutation():\n",
    "    a = list('abbced')\n",
    "    for perm in list(permutations(list(range(len(a)))))[:100]:\n",
    "        assert a == apply_permutation(\n",
    "            apply_permutation(a, perm), inverse_permutation(perm))\n",
    "\n",
    "\n",
    "# test_permutation()\n",
    "\n",
    "from adaptive import AdaptiveHuffman\n",
    "class Huffman:\n",
    "    def encode(l: list):\n",
    "        ada_huff = AdaptiveHuffman(bytes(l))\n",
    "        return ada_huff.encode()\n",
    "\n",
    "    def decode(code: bitarray):\n",
    "        add_huff_decoder = AdaptiveHuffman(code)\n",
    "        return add_huff_decoder.decode()\n",
    "    \n",
    "\n",
    "def test_huffman():\n",
    "    for i in range(10):\n",
    "        l = [random.randint(3,9) for _ in range(10)]\n",
    "        assert l == Huffman.decode(Huffman.encode(l))\n",
    "\n",
    "# test_huffman()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitarray('00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001011')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gamma_code(number_bits):\n",
    "    number = int(''.join(map(str, number_bits)), 2)\n",
    "    # number += 1  # кодируем число на 1 больше чтобы могли подавать нули\n",
    "    i = math.floor(math.log(number, 2))\n",
    "    return bitarray([0]*i + [int(c) for c in \"{0:b}\".format(number)[-(i+1):]])\n",
    "\n",
    "\n",
    "def delta_code(number_bits):\n",
    "    number = int(''.join(map(str, number_bits)), 2)\n",
    "    i = math.floor(math.log(number, 2))\n",
    "    try:\n",
    "        diap = gamma_code(\"{0:b}\".format(i+1))\n",
    "    except:\n",
    "        raise ValueError(i)\n",
    "    return diap+bitarray([int(c) for c in \"{0:b}\".format(number)[-(i+1):]])\n",
    "\n",
    "\n",
    "def list_int_to(l, func_code):\n",
    "    res = bitarray()\n",
    "    for c in l:\n",
    "        res.extend(func_code(\"{0:b}\".format(c)))\n",
    "    return res\n",
    "\n",
    "def gamma_to_list_int(ar: bitarray):\n",
    "    j = 0\n",
    "    res = []\n",
    "    zeros_cnt = 0\n",
    "    while j < len(ar):\n",
    "        if ar[j] == 0:\n",
    "            zeros_cnt += 1\n",
    "            j += 1\n",
    "            continue\n",
    "        number_bits = ar[j:j+zeros_cnt+1]\n",
    "        number = int(number_bits.to01(), 2)\n",
    "        res.append(number)\n",
    "        j = j + zeros_cnt+1\n",
    "        zeros_cnt = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def list_int_to_gamma(l):\n",
    "    res = bitarray()\n",
    "    for c in l:\n",
    "        gamma_coded = gamma_code(\"{0:b}\".format(c))\n",
    "        res.extend(gamma_coded)\n",
    "    return res\n",
    "\n",
    "\n",
    "def overflow_code(x, scheme):\n",
    "    \"\"\"scheme == [4, 8, 8] ~ 4 + 8 + 8\"\"\"\n",
    "    exp = scheme[0]\n",
    "    upper_bound = 2**exp - 2\n",
    "    if 0 <= x <= upper_bound:\n",
    "        return \"{0:b}\".format(x).rjust(exp, '0')\n",
    "    ones_cnt = exp  # кол-во единиц перед началом числа\n",
    "    for exp in scheme[1:]:\n",
    "        prev_upper_bound = upper_bound\n",
    "        upper_bound = upper_bound + 2**exp - 1\n",
    "        if x <= upper_bound:\n",
    "            return '1'*ones_cnt + \"{0:b}\".format(x-(prev_upper_bound+1)).rjust(exp, '0')\n",
    "        ones_cnt += exp\n",
    "    raise ValueError(f\"Scheme {scheme} is not enough to code {x}\")\n",
    "\n",
    "\n",
    "def overflow_array(l):\n",
    "    res = bitarray()\n",
    "    for x in l:\n",
    "        res.extend(overflow_code(x, [2, 4, 16]))\n",
    "    return res\n",
    "\n",
    "HEADER_SIZE = 128\n",
    "def get_header(x):\n",
    "    \"\"\"x - length of huffman compressed data\"\"\"\n",
    "    return bitarray(\"{0:b}\".format(x).rjust(HEADER_SIZE, '0'))\n",
    "\n",
    "\n",
    "def test_gamma():\n",
    "    for i in range(10):\n",
    "        l = [random.randint(3, 9) for _ in range(100)]\n",
    "        assert l == gamma_to_list_int(list_int_to_gamma(l))\n",
    "\n",
    "test_gamma()\n",
    "get_header(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"11\".rjust(1, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rle_compressed, cntrs = rle.encode(mtf.encode(BWT.encode(part_text+SHARP)))\n",
    "from functools import partial\n",
    "global_cntrs = None\n",
    "global_rle_compressed = None\n",
    "global_mtf_encoded = None\n",
    "global_huffman_compressed = None\n",
    "global_huffman_len = None\n",
    "global_cntrs_compressed = None\n",
    "\n",
    "\n",
    "class Archiver:\n",
    "    def encode(text):\n",
    "        global global_cntrs\n",
    "        global global_cntrs_compressed\n",
    "        global global_rle_compressed\n",
    "        global global_mtf_encoded\n",
    "        global global_huffman_compressed\n",
    "        global global_huffman_len\n",
    "        if '$' in text:\n",
    "            raise ValueError('Text should not contain $')\n",
    "        if '#' in text:\n",
    "            raise ValueError('Text should not contain #')\n",
    "        t = text + SHARP\n",
    "        mtf_encoded = mtf.encode(BWT.encode(t))\n",
    "        global_mtf_encoded = mtf_encoded\n",
    "        global_mtf_encoded = mtf_encoded\n",
    "        rle_compressed, cntrs = rle.encode(mtf_encoded)\n",
    "        global_rle_compressed = rle_compressed\n",
    "        huffman_compressed = Huffman.encode(rle_compressed)\n",
    "        global_huffman_compressed = huffman_compressed\n",
    "        huffman_len = len(huffman_compressed)\n",
    "        global_huffman_len = huffman_len\n",
    "        header = get_header(huffman_len)\n",
    "        cntrs_compressed = list_int_to_gamma(cntrs)\n",
    "        global_cntrs_compressed = cntrs_compressed\n",
    "        global_cntrs = cntrs\n",
    "        return header+huffman_compressed+cntrs_compressed\n",
    "\n",
    "    def decode(ar: bitarray):\n",
    "        header = ar[:HEADER_SIZE]\n",
    "        huffman_len = int(header.to01(), 2)\n",
    "        assert huffman_len == global_huffman_len, f\"{huffman_len} {global_huffman_len}\"\n",
    "        huffman_compressed = ar[HEADER_SIZE:HEADER_SIZE+huffman_len]\n",
    "        assert huffman_compressed == global_huffman_compressed\n",
    "        cntrs_compressed = ar[HEADER_SIZE+huffman_len:]\n",
    "        assert cntrs_compressed == global_cntrs_compressed\n",
    "        cntrs = gamma_to_list_int(cntrs_compressed)\n",
    "        rle_compressed = Huffman.decode(huffman_compressed)\n",
    "        assert rle_compressed==global_rle_compressed\n",
    "        mtf_encoded = rle.decode(rle_compressed, cntrs)\n",
    "        assert mtf_encoded == global_mtf_encoded\n",
    "        return BWT.decode(mtf.decode(mtf_encoded))[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (122, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (122, 0)\n",
      "mtf res max & min: (121, 0)\n",
      "mtf res max & min: (122, 0)\n"
     ]
    }
   ],
   "source": [
    "def test_archiver():\n",
    "    for i in range(10):\n",
    "        w = random_subtext(text, 1000)#+SHARP\n",
    "        # print(w)\n",
    "        assert w == Archiver.decode(Archiver.encode(w))\n",
    "test_archiver()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtf res max & min: (122, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29408976854856156"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_text = text#random_subtext(text, 100_000)#text[:10_000]#'ababcabca'\n",
    "bitar = Archiver.encode(tiny_text)\n",
    "decompressed = Archiver.decode(bitar)\n",
    "# print(decompressed)\n",
    "# cntrs\n",
    "(len(bitar)/8)/len(tiny_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
